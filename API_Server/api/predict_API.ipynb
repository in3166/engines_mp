{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#상태유지 스택 순환신경망\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from keras.backend import tensorflow_backend as K\n",
    "from keras import regularizers\n",
    "import os\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "\n",
    "from pandas import DataFrame   \n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Bidirectional, Dropout\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array\n",
    " \n",
    "# date-time parsing function for loading the dataset\n",
    "def parser(x):\n",
    "\treturn datetime.strptime('190'+x, '%Y-%m')\n",
    " \n",
    "# convert time series into supervised learning problem\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    " \n",
    "# create a differenced series / 데이터를 정지하게 하는 기능\n",
    "def difference(dataset, interval=1):\n",
    "\tdiff = list() #빈 리스트 생성 -> 간격만큼 뺀 값들, 작업하기 더 간단한 표현\n",
    "\tfor i in range(interval, len(dataset)): #1부터 데이터셋 길이만큼 반복\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "\treturn Series(diff)\n",
    " \n",
    "# transform series into train and test sets for supervised learning\n",
    "def prepare_data(series, n_test, n_lag, n_seq):\n",
    "\traw_values = series.values\n",
    "\tdiff_series = difference(raw_values, 1)\n",
    "\tdiff_values = diff_series.values\n",
    "\tdiff_values = diff_values.reshape(len(diff_values), 1)\n",
    "    \n",
    "\t# rescale values to -1, 1 - 정규화 minmax\n",
    "\tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\tscaled_values = scaler.fit_transform(diff_values)\n",
    "\tscaled_values = scaled_values.reshape(len(scaled_values), 1)\n",
    "    #scaled_values: 차이만큼 뺴고 정규화한 값을 지도핛ㅂ\n",
    "\tsupervised = series_to_supervised(scaled_values, n_lag, n_seq)\n",
    "\tsupervised_values = supervised.values\n",
    "\ttrain, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n",
    "\treturn scaler, train, test\n",
    " \n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, n_lag, n_seq, n_batch, nb_epoch, n_neurons):\n",
    "\tX, y = train[:, 0:n_lag], train[:, n_lag:]\n",
    "\tX = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "\t# design network\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2])\n",
    "                 , return_sequences=True,stateful=True, recurrent_dropout=0.2))\n",
    "\tmodel.add(LSTM(90, return_sequences=True, stateful=True, recurrent_dropout=0.2))\n",
    "\tmodel.add(Bidirectional(LSTM(60, return_sequences = False)))\n",
    "\tmodel.add(Dense(y.shape[1]))\n",
    "    \n",
    "\tmodel.compile(loss='mse', optimizer='RMSprop')\n",
    "\tprint(model.summary())\n",
    "\t# fit network\n",
    "\tfor i in range(nb_epoch):\n",
    "\t\tprint(\"Total epoch: %d / %d\" % (i+1,nb_epoch))\n",
    "\t\tmodel.fit(X, y, epochs=1, batch_size=n_batch, verbose=1, shuffle=False)\n",
    "\t\tmodel.reset_states()\n",
    "\treturn model\n",
    " \n",
    "# make one forecast with an LSTM,\n",
    "def forecast_lstm(model, X, n_batch):\n",
    "\tX = X.reshape(1, 1, len(X))\n",
    "\tforecast = model.predict(X, batch_size=n_batch)\n",
    "\treturn [x for x in forecast[0, :]]\n",
    " \n",
    "# evaluate the persistence model\n",
    "def make_forecasts(model, n_batch, train, test, n_lag, n_seq):\n",
    "\tforecasts = list()\n",
    "\tfor i in range(len(test)):\n",
    "\t\tX, y = test[i, 0:n_lag], test[i, n_lag:]\n",
    "\t\tforecast = forecast_lstm(model, X, n_batch)\n",
    "\t\tforecasts.append(forecast)\n",
    "\treturn forecasts\n",
    " \n",
    "# invert differenced forecast\n",
    "def inverse_difference(last_ob, forecast):\n",
    "\tinverted = list()\n",
    "\tinverted.append(forecast[0] + last_ob)\n",
    "\tfor i in range(1, len(forecast)):\n",
    "\t\tinverted.append(forecast[i] + inverted[i-1])\n",
    "\treturn inverted\n",
    " \n",
    "# inverse data transform on forecasts\n",
    "def inverse_transform(series, forecasts, scaler, n_test):\n",
    "\tinverted = list()\n",
    "\tfor i in range(len(forecasts)):\n",
    "\t\tforecast = array(forecasts[i])\n",
    "\t\tforecast = forecast.reshape(1, len(forecast))\n",
    "\t\tinv_scale = scaler.inverse_transform(forecast)\n",
    "\t\tinv_scale = inv_scale[0, :]\n",
    "\t\tindex = len(series) - n_test + i - 1\n",
    "\t\tlast_ob = series.values[index]\n",
    "\t\tinv_diff = inverse_difference(last_ob, inv_scale)\n",
    "\t\tinverted.append(inv_diff)\n",
    "\treturn inverted\n",
    " \n",
    "# evaluate the RMSE for each forecast time step\n",
    "def evaluate_forecasts(test, forecasts, n_lag, n_seq):\n",
    "\tfor i in range(n_seq):\n",
    "\t\tactual = [row[i] for row in test]\n",
    "\t\tpredicted = [forecast[i] for forecast in forecasts]\n",
    "\t\trmse = sqrt(mean_squared_error(actual, predicted))\n",
    "\t\t#print('t+%d RMSE: %f' % ((i+1), rmse))\n",
    " \n",
    "# plot the forecasts in the context of the original dataset\n",
    "def plot_forecasts(series, forecasts, n_test, y):\n",
    "\tplt.figure(figsize=(25,5))\n",
    "\tplt.xticks(np.arange(0,series.shape[0],step=500),series.index, rotation=90)\n",
    "\tpyplot.plot(series.values)\n",
    "\tfor i in range(len(forecasts)):\n",
    "\t\toff_s = len(series) - n_test + i - 1\n",
    "\t\toff_e = off_s + len(forecasts[i]) + 1\n",
    "\t\txaxis = [x for x in range(off_s, off_e)]\n",
    "\t\tyaxis = [series.values[off_s]] + forecasts[i]\n",
    "\t\tplt.axhline(y=y, xmin=0.02, xmax=0.98, color='red')\n",
    "\t\tpyplot.plot(xaxis, yaxis, color='orange')\n",
    "pyplot.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file():\n",
    "    files1 = glob.glob(os.path.join('../../', '*.csv'))\n",
    "    #files = os.path.listdir('./')\n",
    "    #train = pd.DataFrame()\n",
    "    #xy = pd.DataFrame()\n",
    "\n",
    "    li = []\n",
    "    print(len(files1))\n",
    "    for file in range(len(files1)-300):\n",
    "        df = pd.read_csv(files1[file], error_bad_lines=False, header=0, index_col=False, usecols=[*range(0, 198)],\n",
    "            names=[\"DATE\",\"EA1\",\"EA2\",\"EA3\",\"EA4\",\"EA5\",\"EA6\",\"EA7\",\"EA8\",\"EA9\",\"EA10\",\"EA11\",\"EA12\",\"EA13\",\"EA14\",\"EA15\",\"EA16\",\"EA17\",\"EA18\",\"EA19\",\"EA20\",\"EA21\",\"EA22\",\"EA23\",\"EA24\",\"EA25\",\"EA26\",\"EA27\",\"EA28\",\"EA29\",\"EA30\",\"EA31\",\"EA32\",\"EA33\",\"EA34\",\"EA35\",\"EA36\",\"EA37\",\"EA38\",\"EA39\",\"EA40\",\"EA41\",\"EA42\",\"EA43\",\"EA44\",\"EA45\",\"EA46\",\"EA47\",\"EA48\",\"EA49\",\"EA50\",\"EA51\",\"EA52\",\"EA53\",\"EA54\",\"EA55\",\"EA56\",\"EA57\",\"EA58\",\"EA59\",\"EA60\",\"EA61\",\"EA62\",\"EA63\",\"EA64\",\"EA65\",\"EA66\",\"EA67\",\"EA68\",\"EA69\",\"EA70\",\"EA71\",\"EA72\",\"EA73\",\"EA74\",\"EA75\",\"EA76\",\"EA77\",\"EA78\",\"EA79\",\"EA80\",\"EA81\",\"EA82\",\"EA83\",\"EA84\",\"EA85\",\"EA86\",\"EA87\",\"EA88\",\"EA89\",\"EA90\",\"EA91\",\"EA92\",\"EA93\",\"EA94\",\"EA95\",\"EA96\",\"EA97\",\"EA98\",\"EA99\",\"EA100\",\"EA101\",\"EA102\",\"EA103\",\"EA104\",\"EA105\",\"EA106\",\"EA107\",\"EA108\",\"EA109\",\"EA110\",\"EA111\",\"EA112\",\"EA113\"]\n",
    "            )   #오류나는 라인 생략, header=0: 1번째 행이 칼럼 이름\n",
    "\n",
    "        li.append(df)\n",
    "    xy = pd.concat(li, axis=0, ignore_index=True)\n",
    "    return xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_api():\n",
    "        xy = read_file()\n",
    "        date = xy.iloc[27000:33000, 0:1].values\n",
    "        value = xy.iloc[27000:33000, 89:90].values\n",
    "\n",
    "        date = date.ravel()\n",
    "        date1 = date\n",
    "        value = value.ravel()\n",
    "\n",
    "\n",
    "        x1 = pd.DataFrame({xy.columns[89]: value},index=date)\n",
    "\n",
    "        #value = std_based_outlier(value)\n",
    "        #xy = MinMaxScaler(xy)\n",
    "        #xy = pd.DataFrame(list(xy))\n",
    "\n",
    "        #지수 이동평균 EMA (Exponetial Moving Average)\n",
    "        origin_x =x1\n",
    "        x1 = x1.ewm(170).mean()\n",
    "\n",
    "        n_lag = 299 #모델링에 사용 된 지연 수\n",
    "        n_seq = 300  #지속할 예측 단계의 수\n",
    "        n_test = 1\n",
    "        n_epochs = 10\n",
    "        n_batch = 100\n",
    "        n_neurons = 100\n",
    "\n",
    "        # prepare data\n",
    "        # 데이터를 변경하고 크기를 재조정한 다음, 지속성(persistence) 예제를 사용하여 감독된 학습 문제로 변환을 수행하고 테스트 세트를 훈련\n",
    "        scaler, train, test = prepare_data(x1, n_test, n_lag, n_seq)\n",
    "\n",
    "        # fit model\n",
    "        model = fit_lstm(train, n_lag, n_seq, n_batch, n_epochs, n_neurons)\n",
    "\n",
    "        #다른 배치사이즈 적용\n",
    "        n_batch = 1\n",
    "        X1, y1 = train[:, 0:n_lag], train[:, n_lag:]\n",
    "        X1 = X1.reshape(X1.shape[0], 1, X1.shape[1])\n",
    "\n",
    "        new_model =  Sequential()\n",
    "        new_model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X1.shape[1], X1.shape[2])\n",
    "                          ,return_sequences=True, stateful=True, dropout=0.2))\n",
    "        new_model.add(LSTM(90, return_sequences=True,stateful=True, dropout=0.2))\n",
    "        new_model.add(Bidirectional(LSTM(60, return_sequences = False)))\n",
    "        new_model.add(Dense(y1.shape[1]))\n",
    "\n",
    "        old_weights = model.get_weights()\n",
    "        new_model.set_weights(old_weights)\n",
    "\n",
    "        new_model.compile(loss='mse', optimizer='RMSprop')\n",
    "\n",
    "\n",
    "        from keras.models import load_model\n",
    "        new_model.save('model1.h5')\n",
    "        # make forecasts\n",
    "        forecasts = make_forecasts(new_model, n_batch, train, test, n_lag, n_seq)\n",
    "\n",
    "        forecasts = inverse_transform(x1, forecasts, scaler, n_test+2)\n",
    "        actual = [row[n_lag:] for row in test]\n",
    "        actual = inverse_transform(x1, actual, scaler, n_test+2)\n",
    "\n",
    "        # evaluate forecasts\n",
    "        evaluate_forecasts(actual, forecasts, n_lag, n_seq)\n",
    "\n",
    "        # make forecasts\n",
    "        forecasts = make_forecasts(new_model, n_batch, train, test, n_lag, n_seq)\n",
    "\n",
    "        # inverse transform forecasts and test\n",
    "        forecasts = inverse_transform(x1, forecasts, scaler, n_test+2)\n",
    "        actual = [row[n_lag:] for row in test]\n",
    "        actual = inverse_transform(x1, actual, scaler, n_test+2)\n",
    "\n",
    "        # evaluate forecasts\n",
    "        evaluate_forecasts(actual, forecasts, n_lag, n_seq)\n",
    "\n",
    "        #plot_forecasts(x1, forecasts, n_test+2, 510.1)\n",
    "        a1 = []\n",
    "\n",
    "        for i in range(len(forecasts[0])):\n",
    "            a1.append(forecasts[0][i][0])   \n",
    "\n",
    "        x1 = x1.to_numpy()\n",
    "        x1 = x1.flatten().tolist()\n",
    "    \n",
    "        origin_x = origin_x.to_numpy()\n",
    "        origin_x = origin_x.flatten().tolist()\n",
    "\n",
    "        date1 = date1.tolist()\n",
    "        date1 = date1[5000:]\n",
    "        x1 = x1[5000:]\n",
    "\n",
    "        return date1, x1, origin_x, a1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3e592c4a5b47bd41395c84b4363f15991f7bccf133fc99f5f27935a994dd51d8"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('tensorflow': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
